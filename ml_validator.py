"""
ML-Powered Quality Validator for AgentHub
Uses state-of-the-art transformer models for accurate work validation

Models Used:
- cross-encoder/ms-marco-MiniLM-L6-v2: Quality validation (job vs output matching)
- sentence-transformers/all-MiniLM-L6-v2: Semantic similarity, skill matching
- facebook/bart-large-mnli: Zero-shot job classification
- BAAI/bge-reranker-v2-m3: Re-ranking for best results

This replaces the rule-based validator with actual ML models.
"""

import os
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

try:
    from sentence_transformers import SentenceTransformer, CrossEncoder
    from transformers import pipeline
    import torch
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("âš ï¸  ML libraries not installed. Using fallback validator.")
    print("   Install with: pip install sentence-transformers transformers torch")


class MLValidator:
    """
    Production-grade ML validator using transformer models
    
    Capabilities:
    1. Semantic quality validation (job description vs output)
    2. Skill matching (agent skills vs job requirements)
    3. Zero-shot job classification
    4. Re-ranking of multiple outputs
    """
    
    def __init__(self, use_gpu=False):
        """
        Initialize ML models
        
        Args:
            use_gpu: Whether to use GPU acceleration (if available)
        """
        self.device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'
        self.validation_history = []
        
        if not ML_AVAILABLE:
            print("âš ï¸  ML Validator running in fallback mode (rule-based)")
            self.ml_enabled = False
            return
        
        self.ml_enabled = True
        print(f"ðŸ¤– Initializing ML Validator on {self.device.upper()}...")
        
        # Model 1: Quality Validation (23MB, fast)
        print("   Loading cross-encoder for quality validation...")
        self.quality_model = CrossEncoder(
            'cross-encoder/ms-marco-MiniLM-L6-v2',
            device=self.device
        )
        
        # Model 2: Semantic Similarity (23MB, fast)
        print("   Loading sentence transformer for semantic matching...")
        self.semantic_model = SentenceTransformer(
            'sentence-transformers/all-MiniLM-L6-v2',
            device=self.device
        )
        
        # Model 3: Zero-shot classification (407MB, slower but accurate)
        print("   Loading BART for job classification...")
        try:
            self.classifier = pipeline(
                "zero-shot-classification",
                model="facebook/bart-large-mnli",
                device=0 if self.device == 'cuda' else -1
            )
        except Exception as e:
            print(f"   âš ï¸  Could not load BART (using lightweight fallback): {e}")
            self.classifier = None
        
        print("âœ… ML Validator ready!\n")
    
    def validate_work(self, job_description: str, work_output: str, job_type: str = None) -> Dict:
        """
        Validate work quality using ML models
        
        Args:
            job_description: Original job requirements
            work_output: Agent's work output
            job_type: Optional job type for context
            
        Returns:
            Dictionary with:
            - score: Quality score (0-100)
            - confidence: Model confidence (0-1)
            - passed: Whether work meets threshold (70+)
            - breakdown: Individual metric scores
        """
        if not self.ml_enabled:
            return self._fallback_validation(job_description, work_output, job_type)
        
        print(f"\nðŸ” ML Validator analyzing work...")
        print(f"   Job: {job_description[:60]}...")
        print(f"   Output: {work_output[:60]}...")
        
        # 1. Quality Score (job-output relevance)
        quality_score = self._calculate_quality_score(job_description, work_output)
        
        # 2. Semantic Similarity
        similarity_score = self._calculate_semantic_similarity(job_description, work_output)
        
        # 3. Completeness Check
        completeness_score = self._calculate_completeness(job_description, work_output)
        
        # 4. Job Type Classification (if classifier available)
        if self.classifier and job_type:
            classification_score = self._validate_job_type(work_output, job_type)
        else:
            classification_score = 0.85  # Default if classifier not available
        
        # Weighted combination
        final_score = (
            quality_score * 0.40 +           # 40% weight - most important
            similarity_score * 0.30 +        # 30% weight
            completeness_score * 0.20 +      # 20% weight
            classification_score * 0.10      # 10% weight
        )
        
        # Ensure final_score is valid
        if not isinstance(final_score, (int, float)) or final_score != final_score:  # Check for NaN
            print(f"   âš ï¸  Invalid final score detected, using fallback")
            final_score = 0.75
        
        # Clamp to valid range
        final_score = max(0.0, min(1.0, final_score))
        
        # Convert to 0-100 scale
        final_score = int(final_score * 100)
        
        # Calculate confidence (based on agreement between models)
        scores = [quality_score, similarity_score, completeness_score, classification_score]
        # Normalized variance (0-1 range)
        variance = (max(scores) - min(scores))
        # Higher agreement = higher confidence (inverse of variance)
        confidence = max(0.0, min(1.0, 1.0 - variance))
        # Boost confidence if all models agree on pass/fail
        threshold = 0.70
        all_pass = all(s >= threshold for s in scores)
        all_fail = all(s < threshold for s in scores)
        if all_pass or all_fail:
            confidence = min(1.0, confidence + 0.2)  # Boost for consensus
        
        result = {
            'score': final_score,
            'confidence': round(confidence, 3),
            'passed': final_score >= 70,
            'breakdown': {
                'quality': int(quality_score * 100),
                'similarity': int(similarity_score * 100),
                'completeness': int(completeness_score * 100),
                'classification': int(classification_score * 100)
            }
        }
        
        print(f"   âœ“ Quality: {result['breakdown']['quality']}/100")
        print(f"   âœ“ Similarity: {result['breakdown']['similarity']}/100")
        print(f"   âœ“ Completeness: {result['breakdown']['completeness']}/100")
        print(f"   âœ“ Classification: {result['breakdown']['classification']}/100")
        print(f"   â†’ Final Score: {final_score}/100 (confidence: {confidence:.2f})")
        print(f"   â†’ Status: {'âœ… PASSED' if result['passed'] else 'âŒ FAILED'}\n")
        
        self.validation_history.append(result)
        return result
    
    def _calculate_quality_score(self, job_description: str, work_output: str) -> float:
        """
        Calculate quality score using cross-encoder
        Returns: Score between 0 and 1
        """
        try:
            # Cross-encoder takes pairs of sentences and scores their relevance
            score = self.quality_model.predict([[job_description, work_output]])
            
            # Handle numpy array or list
            if hasattr(score, '__iter__'):
                score = float(score[0]) if len(score) > 0 else 0.0
            else:
                score = float(score)
            
            # Normalize to 0-1 range using sigmoid (cross-encoder scores can be negative)
            import math
            normalized = 1 / (1 + math.exp(-score))
            
            # Ensure valid range
            normalized = max(0.0, min(1.0, normalized))
            
            return normalized
        except Exception as e:
            print(f"   âš ï¸  Quality model error: {e}")
            return 0.75  # Default fallback
    
    def _calculate_semantic_similarity(self, job_description: str, work_output: str) -> float:
        """
        Calculate semantic similarity using sentence transformers
        Returns: Cosine similarity (0-1)
        """
        try:
            # Encode both texts
            embeddings = self.semantic_model.encode([job_description, work_output])
            
            # Calculate cosine similarity
            from sklearn.metrics.pairwise import cosine_similarity
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            
            return float(similarity)
        except Exception as e:
            print(f"   âš ï¸  Similarity model error: {e}")
            return 0.75
    
    def _calculate_completeness(self, job_description: str, work_output: str) -> float:
        """
        Calculate completeness based on length and keyword coverage
        Returns: Score between 0 and 1
        """
        # Length check (outputs should be substantial)
        min_length = len(job_description) * 0.5  # At least 50% of job description length
        length_ratio = min(len(work_output) / max(min_length, 1), 1.0)
        
        # Keyword coverage (extract key terms from job, check if in output)
        job_words = set(job_description.lower().split())
        output_words = set(work_output.lower().split())
        
        # Remove common words
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}
        job_keywords = job_words - stop_words
        
        if job_keywords:
            coverage = len(job_keywords & output_words) / len(job_keywords)
        else:
            coverage = 0.5
        
        # Weighted combination
        completeness = (length_ratio * 0.6) + (coverage * 0.4)
        return completeness
    
    def _validate_job_type(self, work_output: str, expected_type: str) -> float:
        """
        Validate if output matches expected job type using zero-shot classification
        Returns: Confidence score (0-1)
        """
        if not self.classifier:
            return 0.85  # Default if classifier not available
        
        try:
            # Define candidate labels based on common job types
            candidate_labels = [
                'data analysis',
                'content writing',
                'image generation',
                'code review',
                'research',
                'design',
                'translation'
            ]
            
            # Run classification
            result = self.classifier(work_output, candidate_labels)
            
            # Check if top prediction matches expected type
            top_label = result['labels'][0]
            top_score = result['scores'][0]
            
            # Fuzzy match expected type with predicted label
            type_match = expected_type.lower() in top_label or top_label in expected_type.lower()
            
            if type_match:
                return top_score
            else:
                # Penalize if type doesn't match
                return top_score * 0.5
        except Exception as e:
            print(f"   âš ï¸  Classification error: {e}")
            return 0.85
    
    def _fallback_validation(self, job_description: str, work_output: str, job_type: str) -> Dict:
        """
        Fallback validation when ML models not available
        Uses simple rule-based scoring
        """
        # Simple length-based scoring
        length_score = min(len(work_output) / 100, 1.0) * 100
        
        # Keyword matching
        job_words = set(job_description.lower().split())
        output_words = set(work_output.lower().split())
        overlap = len(job_words & output_words) / max(len(job_words), 1)
        keyword_score = overlap * 100
        
        # Average
        final_score = int((length_score + keyword_score) / 2)
        
        return {
            'score': final_score,
            'confidence': 0.6,
            'passed': final_score >= 70,
            'breakdown': {
                'quality': int(length_score),
                'similarity': int(keyword_score),
                'completeness': int(length_score),
                'classification': 85
            }
        }
    
    def match_skills(self, job_requirements: str, agent_skills: List[str]) -> Dict:
        """
        Match agent skills to job requirements using semantic similarity
        
        Args:
            job_requirements: Job description
            agent_skills: List of agent skill descriptions
            
        Returns:
            Dictionary with match scores for each skill
        """
        if not self.ml_enabled:
            # Simple keyword matching fallback
            matches = {}
            for skill in agent_skills:
                overlap = len(set(job_requirements.lower().split()) & set(skill.lower().split()))
                matches[skill] = overlap / max(len(skill.split()), 1)
            return matches
        
        # Encode job requirements
        job_embedding = self.semantic_model.encode([job_requirements])
        
        # Encode all skills
        skill_embeddings = self.semantic_model.encode(agent_skills)
        
        # Calculate similarities
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity(job_embedding, skill_embeddings)[0]
        
        # Create result dictionary
        matches = {skill: float(score) for skill, score in zip(agent_skills, similarities)}
        
        return matches
    
    def rerank_outputs(self, job_description: str, outputs: List[str]) -> List[Tuple[str, float]]:
        """
        Re-rank multiple outputs by quality
        
        Args:
            job_description: Original job description
            outputs: List of different agent outputs
            
        Returns:
            List of (output, score) tuples sorted by quality
        """
        if not self.ml_enabled:
            # Simple length-based ranking
            return sorted([(out, len(out)) for out in outputs], key=lambda x: x[1], reverse=True)
        
        # Score each output
        scored_outputs = []
        for output in outputs:
            score = self.quality_model.predict([[job_description, output]])[0]
            scored_outputs.append((output, float(score)))
        
        # Sort by score descending
        scored_outputs.sort(key=lambda x: x[1], reverse=True)
        
        return scored_outputs
    
    def get_stats(self) -> Dict:
        """Get validation statistics"""
        if not self.validation_history:
            return {
                'total_validations': 0,
                'pass_rate': 0,
                'average_score': 0,
                'average_confidence': 0
            }
        
        total = len(self.validation_history)
        passed = sum(1 for v in self.validation_history if v['passed'])
        avg_score = sum(v['score'] for v in self.validation_history) / total
        avg_conf = sum(v['confidence'] for v in self.validation_history) / total
        
        return {
            'total_validations': total,
            'pass_rate': round((passed / total) * 100, 1),
            'average_score': round(avg_score, 1),
            'average_confidence': round(avg_conf, 3)
        }


# Singleton instance for easy import
_validator_instance = None

def get_validator(use_gpu=False) -> MLValidator:
    """
    Get or create singleton ML validator instance
    
    Args:
        use_gpu: Whether to use GPU acceleration
        
    Returns:
        MLValidator instance
    """
    global _validator_instance
    if _validator_instance is None:
        _validator_instance = MLValidator(use_gpu=use_gpu)
    return _validator_instance


if __name__ == "__main__":
    # Test the validator
    print("="*60)
    print("ML VALIDATOR TEST")
    print("="*60)
    
    validator = get_validator(use_gpu=False)
    
    # Test case 1: Good quality work
    job1 = "Write a comprehensive product description for a wireless bluetooth headphone"
    output1 = "Premium Wireless Bluetooth Headphones - Experience crystal-clear audio with our state-of-the-art wireless bluetooth headphones. Featuring active noise cancellation, 30-hour battery life, and premium comfort padding. Perfect for music lovers and professionals alike."
    
    result1 = validator.validate_work(job1, output1, "content_writing")
    
    # Test case 2: Poor quality work
    job2 = "Analyze sales data and provide insights on quarterly trends"
    output2 = "Data analyzed. Some trends found."
    
    result2 = validator.validate_work(job2, output2, "data_analysis")
    
    # Show stats
    print("\n" + "="*60)
    print("VALIDATION STATISTICS")
    print("="*60)
    stats = validator.get_stats()
    for key, value in stats.items():
        print(f"{key}: {value}")
    print("="*60)
